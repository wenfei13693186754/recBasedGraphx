package com.wdcloud.graphx.scalaUtil

import org.apache.hadoop.hbase.HBaseConfiguration
import org.apache.hadoop.hbase.client.HBaseAdmin
import org.apache.hadoop.hbase.client.HTable
import org.apache.hadoop.hbase.client.Put
import org.apache.hadoop.hbase.client.Result
import org.apache.hadoop.hbase.io.ImmutableBytesWritable
import org.apache.hadoop.hbase.mapreduce.TableInputFormat
import org.apache.hadoop.hbase.util.Bytes
import org.apache.spark.SparkContext
import org.apache.spark.graphx.VertexId
import akka.event.slf4j.Logger
import org.apache.spark.graphx.Edge
import collection.JavaConverters._
import org.apache.hadoop.hbase.client.Result
import org.apache.hadoop.hbase.io.ImmutableBytesWritable
/**
 * spark-submit --name "SparkHbase" --master spark://192.168.6.83:7077 --class com.chen.spark.hbase.SparkHbase lib/SparkHbase.jar
 */
object HbaseUtil extends Serializable {

  val logger = Logger(this.getClass.getName)

  //val conf = new SparkConf().setAppName("SparkHbase").setMaster("local")
  //val sc = new SparkContext(conf)
  val conf1 = HBaseConfiguration.create()
  conf1.set("hbase.zookeeper.quorum", "namenode,datanode1,datanode2,datanode3");
  conf1.set("hbase.zookeeper.property.clientPort", "2181");
  conf1.set(TableInputFormat.SCAN_CACHEBLOCKS, "false") //指定扫描出的数据是不是进行缓存，false代表不缓存
  conf1.set(TableInputFormat.SCAN_BATCHSIZE, "100") //指定每次扫描返回的数据量
  conf1.set(TableInputFormat.SCAN_COLUMN_FAMILY, "INFO") //指定扫描的列族|
  /**
   * main
   */
  def main(args: Array[String]): Unit = {
    //read("JANUARY:T_USER_BEHAVIOR", sc)
    //readTableStru(sc)
  }

  /**
   * read
   * 每次只读取100条用户数据
   */
  def read(tableName: String, sc: SparkContext): Unit = {
    conf1.set(TableInputFormat.INPUT_TABLE, tableName)
    val admin = new HBaseAdmin(conf1)
    if (admin.isTableAvailable(tableName)) {
      val hBaseRDD = sc.newAPIHadoopRDD(conf1, classOf[TableInputFormat],
        classOf[org.apache.hadoop.hbase.io.ImmutableBytesWritable],
        classOf[org.apache.hadoop.hbase.client.Result])
      val t1 = System.currentTimeMillis()
      logger.warn("HBaseUtil RDD complete")
      //统计数量
      val count = hBaseRDD.count()
      logger.warn("HbaseUtil HBase RDD Count:" + count)

//    conf1.set(TableInputFormat.INPUT_TABLE, tableName)
//    // Initialize hBase table if necessary,初始化hbase表
//    val admin = new HBaseAdmin(conf1)
//    if (admin.isTableAvailable(tableName)) {
//      
//      val userRDD = sc.newAPIHadoopRDD(conf1, classOf[TableInputFormat],
//        classOf[org.apache.hadoop.hbase.io.ImmutableBytesWritable],
//        classOf[org.apache.hadoop.hbase.client.Result])
//      val t1 = System.currentTimeMillis()
//      logger.warn("HbaseUtil RDD complete")
//      //统计数量
//      val count = userRDD.count()
//      logger.warn("HbaseUtil HBase RDD Count:" + count)
//      var countNum = sc.accumulator(0)
//      var list = List[String]()
//      userRDD.foreach {
//        case (_, result) =>
//          val cellArray = result.raw()
//          val ca = cellArray.map { x => (Bytes.toString(x.getKey), Bytes.toString(x.getValue)) }
//          countNum.+=(1)
//          val USERID = Bytes.toString(result.getValue("INFO".getBytes, "USER_ID".getBytes))
//          list = list.+:(USERID)
//          if (list.size >= 30) {
//            val rest = new SparkSpringRest()
//            rest.invokSparkBySpringRestWithoutResult(list.asJava)
//            list = List[String]()
//          }
//      }
//      if (!list.isEmpty) {
//        val rest = new SparkSpringRest()
//        rest.invokSparkBySpringRestWithoutResult(list.asJava)
//        list = List[String]()
//      }
    }
    admin.close()
  }

  /**
   * 读取hbase表结构
   * 包括某个表的列族和列名
   * 执行结果：
   * 	family是：INFO 列是：01  列值是0.09
      family是：INFO 列是：02  列值是0.07
      family是：INFO 列是：03  列值是0.07
      family是：INFO 列是：04  列值是0.07
      family是：INFO 列是：05  列值是0.07
      family是：INFO 列是：06  列值是0.07
      family是：INFO 列是：07  列值是0.07
      family是：INFO 列是：08  列值是0.07
      family是：INFO 列是：09  列值是0.07
      family是：INFO 列是：10  列值是0.05
      family是：INFO 列是：11  列值是0.05
      family是：INFO 列是：12  列值是0.05
      family是：INFO 列是：13  列值是0.05
      family是：INFO 列是：14  列值是0.05
      family是：INFO 列是：15  列值是0.05
      family是：INFO 列是：16  列值是0.05
      family是：INFO 列是：ACCOUNT  列值是JANUARY
      family是：INFO 列是：RETAIN_DAYS  列值是30
      family是：INFO 列是：USER_ID  列值是RRT
   */
  def readTableStru(sc: SparkContext): Unit = {
    val tableName = "JANUARY:T_USER_CONF"
    val rowKey = "JANUARYRRT"
    conf1.set(TableInputFormat.INPUT_TABLE, tableName)
    conf1.set(TableInputFormat.SCAN_ROW_START, rowKey)
    conf1.set(TableInputFormat.SCAN_ROW_STOP, rowKey)
    // Initialize hBase table if necessary,初始化hbase表
    val admin = new HBaseAdmin(conf1)
    if (admin.isTableAvailable(tableName)) {

      val userRDD = sc.newAPIHadoopRDD(conf1, classOf[TableInputFormat],
        classOf[org.apache.hadoop.hbase.io.ImmutableBytesWritable],
        classOf[org.apache.hadoop.hbase.client.Result])
        
     userRDD.map{x => 
        val result = x._2
        val row = result.rawCells()
        val info = row.map { cell => (Bytes.toString(cell.getFamily), Bytes.toString(cell.getQualifier), Bytes.toString(cell.getValue))}
        println(info.mkString(","))//(INFO,01,0.09),(INFO,02,0.07),(INFO,03,0.07),(INFO,04,0.07),(INFO,05,0.07),(INFO,06,0.07),(INFO,07,0.07),(INFO,08,0.07),(INFO,09,0.07),(INFO,10,0.05),(INFO,11,0.05),(INFO,12,0.05),(INFO,13,0.05),(INFO,14,0.05),(INFO,15,0.05),(INFO,16,0.05),(INFO,ACCOUNT,JANUARY),(INFO,RETAIN_DAYS,30),(INFO,USER_ID,RRT)
      }.collect()
    }
    admin.close()
  }

  /**
   * 将用户对物品的评分写入到hbase表中
   */
  def writeUserItemScore(namespace: String, tableName: String, score: Array[Edge[Double]]) {
    val table = new HTable(conf1, Bytes.toBytes(s"${namespace}:${tableName}"));
    table.setAutoFlush(false, true)
    table.setWriteBufferSize(100000)
    score.foreach { x =>
      val srcId = x.srcId
      val dstId = x.dstId
      val score = x.attr

      val put = new Put((s"${srcId}_${dstId}").getBytes()); //为指定行创建一个Put操作
      put.addColumn("INFO".getBytes(), "USERID".getBytes(), Bytes.toBytes(srcId)); //写入用户要读取多久时间段的数据
      put.addColumn("INFO".getBytes(), "ITEMID".getBytes(), Bytes.toBytes(dstId)); //写入命名空间
      put.addColumn("INFO".getBytes(), "SCORE".getBytes(), Bytes.toBytes(score)); //写入用户业务id 
      table.put(put)
    }
    table.flushCommits()
    table.close();
    println("评分写入成功")
  }

  /**
   * 将基于好友的推荐结果持久化到hbase中
   */
  def writeRecInfoBasedUer(namespace: String, tableName: String, recInfo: Array[(VertexId, (String, Map[String, List[(String, Double)]]))]) {
    val table = new HTable(conf1, Bytes.toBytes(s"${namespace}:${tableName}"));
    table.setAutoFlush(false, true)
    table.setWriteBufferSize(100000)
    recInfo.foreach { x =>
      val userId = x._2._1
      val recInfo = x._2._2.mkString
      val put = new Put((userId).getBytes()); //为指定行创建一个Put操作
      put.addColumn("INFO".getBytes(), "USERID".getBytes(), Bytes.toBytes(userId)); //写入用户要读取多久时间段的数据
      put.addColumn("INFO".getBytes(), "RECINFO".getBytes(), Bytes.toBytes(recInfo)); //写入命名空间
      table.put(put)
    }
    table.flushCommits()
    table.close();
    println("基于好友的推荐结果写入成功")
  }

  /**
   * 将基于圈子的推荐结果持久化到hbase中
   */
  def writeRecInfoBasedCircle(namespace: String, tableName: String, recInfo: Array[(VertexId, (String, Map[String, List[(String, Double)]]))]) {
    val table = new HTable(conf1, Bytes.toBytes(s"${namespace}:${tableName}"));
    table.setAutoFlush(false, true)
    table.setWriteBufferSize(100000)
    recInfo.foreach { x =>
      val userId = x._2._1
      val recInfo = x._2._2.mkString
      val put = new Put((userId).getBytes()); //为指定行创建一个Put操作
      put.addColumn("INFO".getBytes(), "USERID".getBytes(), Bytes.toBytes(userId)); //写入用户要读取多久时间段的数据
      put.addColumn("INFO".getBytes(), "RECINFO".getBytes(), Bytes.toBytes(recInfo)); //写入命名空间
      table.put(put)
    }
    table.flushCommits()
    table.close();
    println("基于圈子的推荐结果写入成功")
  }
}















