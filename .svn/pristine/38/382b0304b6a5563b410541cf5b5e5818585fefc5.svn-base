package com.wdcloud.graphx.modelBuild.graph

import java.io.Serializable

import org.apache.hadoop.hbase.HBaseConfiguration
import org.apache.hadoop.hbase.client.Result
import org.apache.hadoop.hbase.io.ImmutableBytesWritable
import org.apache.hadoop.hbase.mapreduce.TableInputFormat
import org.apache.hadoop.hbase.util.Bytes
import org.apache.spark.HashPartitioner
import org.apache.spark.SparkContext
import org.apache.spark.graphx.Edge
import org.apache.spark.rdd.RDD
import org.apache.spark.rdd.RDD.rddToPairRDDFunctions

import com.google.common.hash.Hashing
import com.wdcloud.graphx.scalaUtil.HbaseUtil

import akka.event.slf4j.Logger  
import com.wdcloud.graphx.javaUtil.Configuration
import org.apache.batik.css.engine.value.css2.OverflowManager
import com.wdcloud.graphx.environmentContext.DataContext

/**
 * 用来进行数据的读取和保存
 */
object ReadData extends DataContext with Serializable {  
  val logger = Logger(this.getClass.getName)
  @transient var conf: Configuration = null
  @transient var userConf: Map[String, String] = null 
  /**
   * 从hbase中读取边数据生成边RDD  
   * param: 
   * 		SparkContext-->spark运行的上下文
   * return： 
   * 		RDD[Edge[Double]]-->生成图使用的边数据，边上放着顶点之间的亲密度
   *
   */
  def readEdgesData(sc: SparkContext): RDD[Edge[Double]] = {
	  //从hbase中读取用户配置信息
	  val edgeTable = userConf.get("user.behavior.table").get   
	  val namespace = userConf.get("namespace").get 
	  val tableName = s"${namespace}:${edgeTable}"    
	  
    //创建边RDD  
	  val endTime = System.currentTimeMillis() + ""
	  val startTime = System.currentTimeMillis() + ""
	  //val startTime = (System.currentTimeMillis() + confInfo.get("RETAIN_DAYS").getOrElse("0").toInt * 24 * 60 * 60 * 1000) + ""
    
    val hbaseConf = HBaseConfiguration.create()
    hbaseConf.set("hbase.zookeeper.quorum", "namenode,datanode1,datanode2,datanode3");
    hbaseConf.set("hbase.zookeeper.property.clientPort", "2181");
    hbaseConf.set(TableInputFormat.SCAN_CACHEBLOCKS, "false") //指定扫描出的数据是不是进行缓存，false代表不缓存
    hbaseConf.set(TableInputFormat.SCAN_BATCHSIZE, "10000") //指定每次扫描返回的数据量
    hbaseConf.set(TableInputFormat.SCAN_COLUMN_FAMILY, "INFO") //指定扫描的列族|
    
    //这里可以设置用户读取数据的时间段
    if (startTime.equals(endTime)) { //如果用户没有传RETAIN_DAYS,那么默认采集所有数据，所以这里不需要设置时间段
      hbaseConf.set(TableInputFormat.INPUT_TABLE, tableName)
    } else {
      hbaseConf.set(TableInputFormat.INPUT_TABLE, tableName) //扫描那张表
      //hbaseConf.set(TableInputFormat.SCAN_TIMERANGE_START, startTime)
      //hbaseConf.set(TableInputFormat.SCAN_TIMERANGE_END, endTime)
    }
    
    val hBaseRDD = sc.newAPIHadoopRDD(hbaseConf, classOf[TableInputFormat],
      classOf[org.apache.hadoop.hbase.io.ImmutableBytesWritable],
      classOf[org.apache.hadoop.hbase.client.Result])  
        
    val edgesData = hBaseRDD.map { //这里是按行键读取的，也就是下边获得的key是一个一个的行健，不是一个行健组成的数组   行健：用户业务id_物品业务id
      case (_, result) =>

        val key = Bytes.toString(result.getRow)
        val srcId = Bytes.toString(result.getValue("INFO".getBytes, "USER_ID".getBytes)) //读出用户业务id
        val dstId = Bytes.toString(result.getValue("INFO".getBytes, "ITEM_ID".getBytes)) //读出用户关系的物品(好友、圈子、物品)业务id
        val last_time = Bytes.toString(result.getValue("INFO".getBytes, "BHV_DATETIME".getBytes)) //读出时间戳
        var bhv_type = Bytes.toString(result.getValue("INFO".getBytes, "ACTION".getBytes)) //读出行为类型
        var dst_category = Bytes.toString(result.getValue("INFO".getBytes, "DST_CATEGORY".getBytes)) //读出dst顶点类别
        var src_category = Bytes.toString(result.getValue("INFO".getBytes, "SRC_CATEGORY".getBytes)) //读出src顶点类别

        //对异常数据的处理
        if(srcId == null|| srcId == "" ||dstId == null || dstId == "" || last_time == null || last_time == ""|| bhv_type == null || bhv_type == ""){
          Edge(0L, 0L, 0.0)
        }
        //对顶点类型没有给出的处理
        if(src_category == null && dst_category != null){//如果用户没有设置src顶点的类型，设置了dst顶点的类型，那么设置src顶点类型默认值为person
          src_category = "person"
        }else if(src_category != null && dst_category == null){//如果用户设置src顶点的类型，设没有置dst顶点的类型，那么设置dst顶点类型默认值为item
        	dst_category = "item"
        }else if(src_category == null && dst_category == null){//如果用户没有设置src顶点和dst顶点类型，那么设置所有顶点默认类型为point
          src_category = "point"
          dst_category = "point"
        }else{
        }
        var src_innerId: Long = hashId(src_category, srcId)
        var dst_innerId: Long = hashId(dst_category, dstId)        
        //读取行为类型
        val relaScore: Double = userConf.get(bhv_type).get.toDouble  
        //计算顶点之间亲密度
        val totalScore: Double = math.log10(relaScore * 1000000 / math.pow(last_time.toLong / 360000000, 1.5) + 1)
        Edge(src_innerId, dst_innerId, totalScore)
    }
    
    //将用户对物品的评分保存到hbase中
    //HbaseUtil.writeUserItemScore(namespace, "T_USER_ITEM_SCORE", edgesData.collect())
    
    logger.info("从hbase中的"+tableName+"表中读取用户边属性信息完成")
    edgesData
  }
  
  /**
   * 读取用户属性的信息
   * 	param: 
   * 		  SparkContext  spark运行的上下文
   *  return： 
   *  		RDD[(Long, Map[String, Object])]   用户顶点组成的RDD,其含义是RDD[用户innerId, Map[用户类型， 用户业务id]]
   */
  def readPersonAttrData(sc: SparkContext): RDD[(Long, Map[String, Object])] = {
    
    //读取配置信息
    val userVertexTable = conf.get("user.property.table")
    val namespace = conf.get("namespace")
    val tableName = s"${namespace}:${userVertexTable}"    
    
    val hbaseConf = HBaseConfiguration.create()
    hbaseConf.set("hbase.zookeeper.quorum", "namenode,datanode1,datanode2,datanode3");
    hbaseConf.set("hbase.zookeeper.property.clientPort", "2181");
    hbaseConf.set(TableInputFormat.SCAN_CACHEBLOCKS, "false") //指定扫描出的数据是不是进行缓存，false代表不缓存
    hbaseConf.set(TableInputFormat.SCAN_BATCHSIZE, "10000") //指定每次扫描返回的数据量
    hbaseConf.set(TableInputFormat.SCAN_COLUMN_FAMILY, "INFO") //指定扫描的列族|
    hbaseConf.set(TableInputFormat.INPUT_TABLE, tableName)

    val hBaseRDD = sc.newAPIHadoopRDD(hbaseConf, classOf[TableInputFormat],
      classOf[org.apache.hadoop.hbase.io.ImmutableBytesWritable],
      classOf[org.apache.hadoop.hbase.client.Result]).partitionBy(new HashPartitioner(10))

    logger.warn("用户属性RDD分区数是: " + hBaseRDD.getNumPartitions)

    val pAttrRDD = hBaseRDD.map { //这里是按行键读取的，也就是下边获得的key是一个一个的行健，不是一个行健组成的数组   行健：用户业务id_物品业务id
      case (_, result) =>
        val key = Bytes.toString(result.getRow)
        val USERID = Bytes.toString(result.getValue("INFO".getBytes, "USER_ID".getBytes)) //读出用户业务id
        if (USERID == null) {
          (0L, Map("type" -> "", "businessId" -> ""))
        } else {
          //生成srcId
          val srcId = hashId("person", USERID)
          //对于圈子作为一个顶点存在的话，使用如下代码
          //将属性添加到map集合中
          var attr: Map[String, Object] = Map("type" -> "person", "businessId" -> USERID)
          (srcId, attr)
        }
    }
    logger.info("从hbase中的"+tableName+"表中读取用户属性信息完成")
    pAttrRDD
  }

  /**
   * 读取用户属性的信息
   * 	param: 
   * 			SparkContext  spark运行的上下文
   *  return：
   *  		RDD[(Long, Map[String, Object])]   物品顶点组成的RDD,其含义是RDD[物品innerId, Map[物品类型， 物品业务id]]
   */
  def readItemAttrData(sc: SparkContext): RDD[(Long, Map[String, Object])] = {
    
    //读取配置信息
    val itemVertexTable = conf.get("item.property.table")
    val namespace = conf.get("namespace")
    val tableName = s"${namespace}:${itemVertexTable}" 
    
    val hbaseConf = HBaseConfiguration.create()
    hbaseConf.set("hbase.zookeeper.quorum", "namenode,datanode1,datanode2,datanode3");
    hbaseConf.set("hbase.zookeeper.property.clientPort", "2181");
    hbaseConf.set(TableInputFormat.SCAN_CACHEBLOCKS, "false") //指定扫描出的数据是不是进行缓存，false代表不缓存
    hbaseConf.set(TableInputFormat.SCAN_BATCHSIZE, "10000") //指定每次扫描返回的数据量
    hbaseConf.set(TableInputFormat.SCAN_COLUMN_FAMILY, "INFO") //指定扫描的列族|
    hbaseConf.set(TableInputFormat.INPUT_TABLE, tableName)
    hbaseConf.set(TableInputFormat.SCAN_TIMERANGE_END, tableName)
    val hBaseRDD = sc.newAPIHadoopRDD(hbaseConf, classOf[TableInputFormat],
      classOf[org.apache.hadoop.hbase.io.ImmutableBytesWritable],
      classOf[org.apache.hadoop.hbase.client.Result]).partitionBy(new HashPartitioner(10))

    logger.warn("物品属性RDD分区数是: " + hBaseRDD.getNumPartitions)

    val iAttrRDD = hBaseRDD.map { //这里是按行键读取的，也就是下边获得的key是一个一个的行健，不是一个行健组成的数组   行健：用户业务id_物品业务id
      case (_, result) =>
        val key = Bytes.toString(result.getRow)
        val ITEM_ID = Bytes.toString(result.getValue("INFO".getBytes, "ITEM_ID".getBytes)) //读出用物品业务id
        val CATEGORY = Bytes.toString(result.getValue("INFO".getBytes, "CATEGORY".getBytes)) //读出物品类别

        if (ITEM_ID == null || CATEGORY == null) {
          (1L, Map("type" -> "", "businessId" -> ""))
        }
        val srcId = hashId(CATEGORY, ITEM_ID)
        //将属性添加到map集合中
        var attr: Map[String, Object] = Map("type" -> CATEGORY, "businessId" -> ITEM_ID)
        (srcId, attr)
    }
    logger.info("从hbase中的"+tableName+"表中读取物品属性信息完成")
    iAttrRDD
  }

  /**
   * 读取用户顶点和物品顶点信息，也就是用户信息和物品信息在一张表中呢
   * 	param: 
   * 			SparkContext  spark运行的上下文
   *  return： 
   *  		RDD[(Long, Map[String, Object])]   用户和物品顶点组成的RDD,其含义是RDD[用户或者物品innerId, Map[用户或物品类型， 用户或物品业务id]]
   */
  def readVertexData(sc: SparkContext): RDD[(Long, Map[String, Object])] = {
    
    //读取配置信息
    val vertexTable = conf.get("vertex.property.table")
    val namespace = conf.get("namespace")
    val tableName = s"${namespace}:${vertexTable}"     
    
    val hbaseConf = HBaseConfiguration.create()
    hbaseConf.set("hbase.zookeeper.quorum", "namenode,datanode1,datanode2,datanode3");
    hbaseConf.set("hbase.zookeeper.property.clientPort", "2181");
    hbaseConf.set(TableInputFormat.SCAN_CACHEBLOCKS, "false") //指定扫描出的数据是不是进行缓存，false代表不缓存
    hbaseConf.set(TableInputFormat.SCAN_BATCHSIZE, "10000") //指定每次扫描返回的数据量
    hbaseConf.set(TableInputFormat.SCAN_COLUMN_FAMILY, "INFO") //指定扫描的列族|
    hbaseConf.set(TableInputFormat.INPUT_TABLE, tableName)

    val hBaseRDD = sc.newAPIHadoopRDD(hbaseConf, classOf[TableInputFormat],
      classOf[org.apache.hadoop.hbase.io.ImmutableBytesWritable],
      classOf[org.apache.hadoop.hbase.client.Result]).partitionBy(new HashPartitioner(10))
    val pAttrRDD = hBaseRDD.map { //这里是按行键读取的，也就是下边获得的key是一个一个的行健，不是一个行健组成的数组   行健：用户业务id_物品业务id
      case (_, result) =>
        val key = Bytes.toString(result.getRow)
        val USERID = Bytes.toString(result.getValue("INFO".getBytes, "USER_ID".getBytes)) //读出用户业务id
        val ITEM_ID = Bytes.toString(result.getValue("INFO".getBytes, "ITEM_ID".getBytes)) //读出物品业务id
        val CATEGORY = Bytes.toString(result.getValue("INFO".getBytes, "CATEGORY".getBytes)) //读出物品业务类别
        //生成srcId
        val srcId = hashId("person", USERID)
        val pAttr: Map[String, Object] = Map("type" -> "person", "businessId" -> USERID)
        //生成物品itemId
        val itemId = hashId(CATEGORY, ITEM_ID)
        val iAttr: Map[String, Object] = Map("type" -> CATEGORY, "businessId" -> ITEM_ID)
        ((srcId, pAttr), (itemId, iAttr))
    }
    pAttrRDD.map(_._1).++(pAttrRDD.map(_._2)).distinct()
  }
  
  /**
   * 读取用户算法配置信息，配置项没有写死，不同的用户的配置项可能不同，这里是读取指定用户的每个配置项
   * 	 param: 
   * 			SparkContext: 代码运行上下文
   *   return：
   *   		Array[(String, String)]   是一个封装了指定用户配置信息的数组
   */
  def readUserConf(sc: SparkContext): Map[String, String] = {  
    
    //获取配置信息
	  val namespace = conf.get("namespace") 
	  val confTable = conf.get("user.conf.table")
	  val rowKey = conf.get("user.business.id")+"_"+conf.get("user.scene.id")    
	  val family = "INFO"
	  
    val hbaseConf = HBaseConfiguration.create()
    hbaseConf.set("hbase.zookeeper.quorum", "namenode,datanode1,datanode2,datanode3");
    hbaseConf.set("hbase.zookeeper.property.clientPort", "2181");  
    hbaseConf.set(TableInputFormat.SCAN_CACHEBLOCKS, "false") //指定扫描出的数据是不是进行缓存，false代表不缓存
    hbaseConf.set(TableInputFormat.SCAN_BATCHSIZE, "10000") //指定每次扫描返回的数据量
    hbaseConf.set(TableInputFormat.SCAN_COLUMN_FAMILY, "INFO") //指定扫描的列族|
    hbaseConf.set(TableInputFormat.INPUT_TABLE, "JANUARY:T_USER_CONF")
    hbaseConf.set(TableInputFormat.SCAN_ROW_START, rowKey)
    hbaseConf.set(TableInputFormat.SCAN_ROW_STOP, rowKey)//这里的SCAN_ROW_START和SCAN_ROW_STOP设置的值一样，是因为我们只从hbase中读取这一个用户的配置信息，所以只对应一行数据
    hbaseConf.set(TableInputFormat.SCAN_COLUMN_FAMILY, family)
    
    val hBaseRDD = sc.newAPIHadoopRDD(hbaseConf, classOf[TableInputFormat],  
      classOf[org.apache.hadoop.hbase.io.ImmutableBytesWritable],
      classOf[org.apache.hadoop.hbase.client.Result]).partitionBy(new HashPartitioner(10))

    val confInfo = hBaseRDD.map { //这里是按行键读取的，也就是下边获得的key是一个一个的行健，不是一个行健组成的数组   行健：用户业务id_物品业务id
      x =>
        val result = x._2
        val row = result.rawCells()//返回支持此Result实例的Cells组成的数组
        row.map { cell => (Bytes.toString(cell.getQualifier), Bytes.toString(cell.getValue)) }

    }.first
    logger.warn("读取配置信息成功")
    userConf = confInfo.toMap
    
    userConf
  }
  
  
  /**
   * 从hbase中读取边数据生成顶点属性RDD，用来在只使用边数据创建图的时候，读取必要的顶点属性
   * param: 
   * 		SparkContext-->spark运行的上下文
   * return： 
   * 		RDD[(Long, Map[String, Object])]-->生成的顶点属性信息
   *
   */
  def readTypeFromEdgeData(sc: SparkContext): RDD[(Long, Map[String, Object])] = {
    
	  //从hbase中读取用户配置信息
	  val namespace = userConf.get("namespace").get  
	  val edgeTable = userConf.get("user.behavior.table").get
	  val tableName = s"${namespace}:${edgeTable}"   
	  
    //创建边RDD
	  val endTime = System.currentTimeMillis() + ""
	  val startTime = System.currentTimeMillis() + ""
	  //val startTime = (System.currentTimeMillis() + confInfo.get("RETAIN_DAYS").getOrElse("0").toInt * 24 * 60 * 60 * 1000) + ""
    
    val hbaseConf = HBaseConfiguration.create()
    hbaseConf.set("hbase.zookeeper.quorum", "namenode,datanode1,datanode2,datanode3");
    hbaseConf.set("hbase.zookeeper.property.clientPort", "2181");
    hbaseConf.set(TableInputFormat.SCAN_CACHEBLOCKS, "false") //指定扫描出的数据是不是进行缓存，false代表不缓存
    hbaseConf.set(TableInputFormat.SCAN_BATCHSIZE, "10000") //指定每次扫描返回的数据量
    hbaseConf.set(TableInputFormat.SCAN_COLUMN_FAMILY, "INFO") //指定扫描的列族|
    hbaseConf.set(TableInputFormat.INPUT_TABLE, "JANUARY:T_USER_BEHAVIOR") //扫描那张表
    
    if (startTime.equals(endTime)) { //如果用户没有传RETAIN_DAYS,那么默认采集所有数据，所以这里不需要设置时间段
      hbaseConf.set(TableInputFormat.INPUT_TABLE, tableName)
    } else {
      hbaseConf.set(TableInputFormat.INPUT_TABLE, tableName) //扫描那张表
      //hbaseConf.set(TableInputFormat.SCAN_TIMERANGE_START, startTime)
      //hbaseConf.set(TableInputFormat.SCAN_TIMERANGE_END, endTime)
    }
    
    val hBaseRDD = sc.newAPIHadoopRDD(hbaseConf, classOf[TableInputFormat],
      classOf[org.apache.hadoop.hbase.io.ImmutableBytesWritable],
      classOf[org.apache.hadoop.hbase.client.Result])
      
    val pointType = hBaseRDD.map { //这里是按行键读取的，也就是下边获得的key是一个一个的行健，不是一个行健组成的数组   行健：用户业务id_物品业务id
      case (_, result) =>

        val key = Bytes.toString(result.getRow)
        val srcId = Bytes.toString(result.getValue("INFO".getBytes, "USER_ID".getBytes)) //读出用户业务id
        val dstId = Bytes.toString(result.getValue("INFO".getBytes, "ITEM_ID".getBytes)) //读出用户关系的物品(好友、圈子、物品)业务id
        var dst_category = Bytes.toString(result.getValue("INFO".getBytes, "DST_CATEGORY".getBytes)) //读出dst顶点类别
        var src_category = Bytes.toString(result.getValue("INFO".getBytes, "SRC_CATEGORY".getBytes)) //读出src顶点类别

        if(src_category == null && dst_category != null){//如果用户没有设置src顶点的类型，设置了dst顶点的类型，那么设置src顶点类型默认值为person
          src_category = "person"
        }else if(src_category != null && dst_category == null){//如果用户设置src顶点的类型，设没有置dst顶点的类型，那么设置dst顶点类型默认值为item
        	dst_category = "item"
        }else if(src_category == null && dst_category == null){//如果用户没有设置src顶点和dst顶点类型，那么设置所有顶点默认类型为point
          src_category = "point"
          dst_category = "point"
        }else{
        }
        var pAttr: Map[String, Object] = Map("type" -> src_category, "businessId" -> srcId)
        var iAttr: Map[String, Object] = Map("type" -> dst_category, "businessId" -> dstId)
        var src_innerId: Long = hashId(src_category, srcId)
        var dst_innerId: Long = hashId(dst_category, dstId) 
        
        ((src_innerId, pAttr), (dst_innerId, iAttr))  
    }
    
    val pt = pointType.map(_._1).++(pointType.map(_._2))
    logger.info("从hbase中的"+tableName+"表中读取顶点类型信息完成")
    pt
  }  

  
  /*
   * 标识不同物品id的工具方法
   */
  def hashId(name: String, str: String) = {
    Hashing.md5().hashString(name + "" + str).asLong()
  }
} 







