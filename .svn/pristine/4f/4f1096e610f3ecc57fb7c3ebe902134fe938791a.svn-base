package com.wdcloud.graphx.recommender.graph.util

import org.apache.spark.rdd.RDD
import org.apache.spark.graphx.Graph
import org.apache.spark.graphx.VertexId

object GraphResultHandle {

  /**
   * 对聚合到顶点上的最终的推荐结果进行处理
   * param:
   * 	oldAttr: Map[String, Object]:每个顶点上的属性值，是map结构，map中使用kv格式保存着多个属性对
   *  newAttr: List[(VertexId, (String, Double, String))]:sendMsg发送来的数据。--->List[(顶点id, (顶点类型, score, 顶点业务id))]
   *  vId: VertexId: 当前顶点hash后的id
   * return:Map[String, List[(String, Double)]]-->Map[推荐物品类型, List[(被推荐物品业务id, score)]]
   */
  def finallyPointResultHandle(oldAttr: Map[String, Object], newAttr: List[(VertexId, (String, Double, String))], vId: VertexId): Map[String, List[(String, Double)]] =  {
    //去除掉，推荐结果中的顶点的邻居顶点
    val neiborIds = oldAttr.apply("neiborId").asInstanceOf[Array[VertexId]]
    val recResult = newAttr.filter(x => !neiborIds.contains(x._1) && vId != x._1)
    //这个时候要对数据进行处理：包括对相同VertexId的数据进行分数累加去重和分组,最终存储的格式是：Map[type, List[(VertexId, Double, 业务id)]]
    val dealData = recResult.groupBy(x => x._1).map { x =>//(VertexId, List[(VertexId, (String, Double, String))])
      val data = x._2.reduce((x, y) => (x._1, (x._2._1, x._2._2 + y._2._2, x._2._3)))
      data._2
    }.groupBy(x => x._1).map(x => (x._1, (x._2.map(y => (y._3, y._2)).toList)))
    
    dealData
  }
  
  
  /**
   * 对pregel迭代出来的结果进行处理
   * 过滤出用户顶点
   * 对那些key是"rec",value值不是Map的赋值为Map[String, List[(String, Double)]]()
   */
  def recResultHandle(graph: Graph[Map[String, Object], Double]): RDD[(VertexId, (String, Map[String, List[(String, Double)]]))] = {
    val resultData = graph.vertices.filter(x => x._2.apply("type").asInstanceOf[String].equals("person"))
      .map(x =>
        try {
          if (x._2.apply("rec").isInstanceOf[Map[String, List[(String, Double)]]]) {
            (x._1, (x._2.apply("businessId").asInstanceOf[String], x._2.apply("rec").asInstanceOf[Map[String, List[(String, Double)]]]))
          } else {
            (x._1, (x._2.apply("businessId").asInstanceOf[String], Map[String, List[(String, Double)]]()))
          }
        } catch {
          case ex: NoSuchElementException => (x._1, ("", Map[String, List[(String, Double)]]()))
        }).cache()

    graph.unpersistVertices(blocking = false)
    graph.edges.unpersist(blocking = false)
    resultData
  }    
  
  /*
   * 将基于好友的推荐结果和基于圈子的推荐结果进行合并,并将合并好的数据放入到kafka中
   * 传来的数据格式：List[(String, Map[String, List[(String, Double)]])]-->List[(用户的业务id, Map[推荐物品类型, List[(推荐物品业务id, score)]])]
   * 返回的结果格式：RDD[(String, String, Map[String, List[(String, Double)]])]-->RDD[(命名空间名称, 用户的业务id, Map[推荐物品类型, List[(推荐物品业务id, score)]])]
   * 
   * 处理过程：
   * 1.首先将传来的基于好友的推荐结果和基于圈子的推荐结果合并为一个List
   * 2.将相同用户id的推荐信息合并为一个map
   * 		(1).将两个推荐结果map先转化为list，然后合并到一起
   * 		(2).对list集合按照类型进行分组
   * 		(3).对分组后内容进行合并和去重
   * 				这里，经过之前的分组，同一个用户的相同类型的推荐结果分到了一组，并组成一个list,所以这里就需要对同一类型的推荐结果组成的list进行处理
   *				list2的数据格式是：Map[String, List[(String, List[(String, Double)])]]-->Map[类型, List[(类型, List[(被推荐物品的业务id, score)])]]
   * 				1.对分组后的由相同类型推荐内容组成的list的数据进行合并，就是将同一种类型的推荐结果(list)合并为一个list
   * 				2.因为合并后的内容是由多个可能存在重复id的被推荐物品组成的，所以这里需要对相同id的数据进行分数累加，并去重
   * 3.将命名空间加入到结果中
   * 4.将结果RDD转化为一个数组
   */
  def combineResult(
    g2: Graph[Map[String, Object], Double],
    nameSpace: String,
    recBasedFriends: RDD[(VertexId, (String, Map[String, List[(String, Double)]]))],
    recBasedCircle: RDD[(VertexId, (String, Map[String, List[(String, Double)]]))]): RDD[(String, String, Map[String, List[(String, Double)]])] = {
    //1.先将两个数组合到一起
    val arr1 = recBasedFriends.++(recBasedCircle)
    //2.将相同用户id的推荐信息(map)合并为一个map,
    //返回结果是;RDD[(String, Map[String, List[(String, Double)]])]-->RDD[(命名空间，用户业务id, Map[推荐物品类型, List[(推荐物品业务id, Double)]])]
    val recResult = arr1.reduceByKey((x, y) => (x._1, mapCombine(x._2, y._2))).map(x => (nameSpace, x._2._1, x._2._2))

    recBasedFriends.unpersist(blocking = false)
    recBasedCircle.unpersist(blocking = false)
    recResult
  }
  
  
  /*
   * 参数代表同一个用户下的基于圈子的推荐信息和基于好友的推荐信息
   * Map[String, List[(String, Double)]]-->Map[type, List[(被推荐物品业务id, score)]]
   * 将两个map进行合并，条件是相同的用户id，对list进行合并，并对合并后的list进行分数累加和去重
   * 返回的是一个新的Map[String, List[(String, Double)]]
   */
  def mapCombine(data1: Map[String, List[(String, Double)]], data2: Map[String, List[(String, Double)]]): Map[String, List[(String, Double)]] = {
    //1.将两个推荐结果map先转化为list，然后合并到一起
    val list1 = data1.toList.++(data2.toList) //List[(String, List[(String, Double)])]
    //2.对list集合按照类型进行分组
    val list2 = list1.groupBy(x => x._1)
    //3.对分组后内容进行合并和去重
    //这里，经过之前的分组，同一个用户的相同类型的推荐结果分到了一组，并组成一个list,所以这里就需要对同一类型的推荐结果组成的list进行处理
    //list2的数据格式是：Map[String, List[(String, List[(String, Double)])]]-->Map[类型, List[(类型, List[(被推荐物品的业务id, score)])]]
    val result = list2.map { x => //x:(String, List[(String, List[(String, Double)])])
      //对分组后的由相同类型推荐内容组成的list的数据进行合并，就是将同一种类型的推荐结果(list)合并为一个list
      //返回值：List[(String, Double)]，代表当前类型下的推荐结果组成的list
      val comData = x._2.map(_._2).reduce((x, y) => x.++(y))
      //因为合并后的内容是由多个可能存在重复id的被推荐物品组成的，所以这里需要对相同id的数据进行分数累加，并去重
      val addScoreData = comData.groupBy(_._1).map(x => (x._1, x._2.map(_._2).reduce((x, y) => x + y))).toList.sortBy(x => x._2).takeRight(10)
      (x._1, addScoreData)
    }

    result
  }
}