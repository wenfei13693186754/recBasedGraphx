package com.wdcloud.graphx.kafka

import java.util.Properties

import kafka.producer.{KeyedMessage, Producer, ProducerConfig}

import scala.io.Source
import scala.reflect.io.Path

/**
  * Kafka Producer
  * 
  * 测试下kafka的produce程序（Scala编写），模拟在应用服务器上日志的收集，看看Flink能否正常消费数据。

		这里的producer实现了每隔3秒去查找特定目录下的文件，将文件的内容批量produce到kafka（async），然后将文件重命名并移动到另外的目录。
  */
class KafkaProduceMsg(brokerList : String, topic : String) extends Runnable{

  private val BROKER_LIST = brokerList //"master:9092,worker1:9092,worker2:9092"
  private val TARGET_TOPIC = topic //"new"
  private val DIR = "/root/Documents/"

  /**
    * 1、配置属性
    * metadata.broker.list : kafka集群的broker，只需指定2个即可
    * serializer.class : 如何序列化发送消息
    * request.required.acks : 1代表需要broker接收到消息后acknowledgment,默认是0  
    * producer.type : 默认就是同步sync
    */
  private val props = new Properties()
  props.put("metadata.broker.list", this.BROKER_LIST)
  props.put("serializer.class", "kafka.serializer.StringEncoder")
  props.put("request.required.acks", "1")
  props.put("producer.type", "async")

  /**
    * 2、创建Producer
    */
  private val config = new ProducerConfig(this.props)
  private val producer = new Producer[String, String](this.config)

  /**
    * 3、产生并发送消息
    * 搜索目录dir下的所有包含“transaction”的文件并将每行的记录以消息的形式发送到kafka
    *
    */
  def run() : Unit = {
    while(true){
      val files = Path(this.DIR).walkFilter(p => p.isFile && p.name.contains("transaction"))

      try{
        for(file <- files){
          val reader = Source.fromFile(file.toString(), "UTF-8")

          for(line <- reader.getLines()){
            val message = new KeyedMessage[String, String](this.TARGET_TOPIC, line)
            producer.send(message)
          }

          //produce完成后，将文件copy到另一个目录，之后delete
          val fileName = file.toFile.name
          //file.toFile.copyTo(Path("/root/Documents/completed/" +fileName + ".completed"))
          file.delete()
        }
      }catch{
        case e : Exception => println(e)
      }

      try{
        //sleep for 3 seconds after send a micro batch of message
        Thread.sleep(3000)
      }catch{
        case e : Exception => println(e)
      }
    }
  }
}

object ProduceMsg {
    def main(args : Array[String]): Unit ={
      val brokerList = "192.168.6.89:9092,192.168.6.83:9092,192.168.6.84:9092" 
      new Thread(new KafkaProduceMsg(brokerList,"TEST_LOGINLOG")).start()
    }
}