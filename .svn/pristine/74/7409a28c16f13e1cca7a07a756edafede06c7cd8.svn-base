package com.wdcloud.graphx.hbase

import org.apache.hadoop.hbase.HBaseConfiguration
import org.apache.hadoop.hbase.client.HBaseAdmin
import org.apache.hadoop.hbase.client.HTable
import org.apache.hadoop.hbase.client.Put
import org.apache.hadoop.hbase.client.Result
import org.apache.hadoop.hbase.io.ImmutableBytesWritable
import org.apache.hadoop.hbase.mapreduce.TableInputFormat
import org.apache.hadoop.hbase.util.Bytes
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

import akka.event.slf4j.Logger
import scala.collection.immutable.Seq
import org.apache.hadoop.hbase.client.Get
import org.apache.hadoop.hbase.client.Scan
import org.apache.spark.rdd.RDD
import com.google.common.hash.Hashing
import org.apache.spark.graphx.Edge
import org.apache.commons.codec.digest.DigestUtils
import java.util.UUID
import spark.client.SparkSpringRest
import collection.JavaConverters._
/**
 * spark-submit --name "SparkHbase" --master spark://192.168.6.83:7077 --class com.chen.spark.hbase.SparkHbase lib/SparkHbase.jar
 */
object HbaseUtil {

  val logger = Logger(this.getClass.getName)

  val conf = new SparkConf().setAppName("SparkHbase").setMaster("local")
  val sc = new SparkContext(conf)
  val conf1 = HBaseConfiguration.create()
  conf1.set("hbase.zookeeper.quorum", "namenode,datanode1,datanode2,datanode3");
  conf1.set("hbase.zookeeper.property.clientPort", "2181");
  conf1.set(TableInputFormat.SCAN_CACHEBLOCKS, "false") //指定扫描出的数据是不是进行缓存，false代表不缓存
  conf1.set(TableInputFormat.SCAN_BATCHSIZE, "100") //指定每次扫描返回的数据量
  conf1.set(TableInputFormat.SCAN_COLUMN_FAMILY, "INFO") //指定扫描的列族|
  /**
   * main
   */  
  def main(args: Array[String]): Unit = {
    read("BATCH2:T_LOG_PERSON", sc)
  }

  /**
   * read
   * 每次只读取100条用户数据
   */
  def read(tableName: String, sc: SparkContext): Unit = {

    conf1.set(TableInputFormat.INPUT_TABLE, tableName)
    // Initialize hBase table if necessary,初始化hbase表
    val admin = new HBaseAdmin(conf1)
    if (admin.isTableAvailable(tableName)) {

      val userRDD = sc.newAPIHadoopRDD(conf1, classOf[TableInputFormat],
        classOf[org.apache.hadoop.hbase.io.ImmutableBytesWritable],
        classOf[org.apache.hadoop.hbase.client.Result])
      val t1 = System.currentTimeMillis()
      logger.warn("RDD complete")
      //统计数量
      val count = userRDD.count()
      logger.warn("HBase RDD Count:" + count)
      var countNum = sc.accumulator(0)
      var list = List[String]()
      userRDD.foreach {
        case (_, result) =>
          countNum.+=(1)
          val USERID = Bytes.toString(result.getValue("INFO".getBytes, "USER_ID".getBytes))
          list = list.+:(USERID)
          if(list.size >=30){
            val rest = new SparkSpringRest()  
		        rest.invokSparkBySpringRestWithoutResult(list.asJava)  
		        list = List[String]()
          }
      }
      if(!list.isEmpty){
        val rest = new SparkSpringRest()  
		        rest.invokSparkBySpringRestWithoutResult(list.asJava)  
		        list = List[String]()
      }
    }
    admin.close()
  }
}