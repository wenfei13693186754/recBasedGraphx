package com.wdcloud.graphx.graphOperate

import java.io.Serializable

import scala.Iterator
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.graphx.Edge
import org.apache.spark.graphx.EdgeDirection
import org.apache.spark.graphx.EdgeTriplet
import org.apache.spark.graphx.Graph
import org.apache.spark.graphx.Graph.graphToGraphOps
import org.apache.spark.graphx.TripletFields
import org.apache.spark.graphx.VertexId
import org.apache.spark.rdd.RDD
import org.apache.spark.rdd.RDD.rddToPairRDDFunctions

import com.google.common.hash.Hashing
import com.wdcloud.graphx.unit.DataToJson
import com.wdcloud.graphx.kafka.Producer
import com.wdcloud.graphx.hbase.SparkHbase
import org.apache.hadoop.hbase.HBaseConfiguration
import org.apache.hadoop.hbase.client.HBaseAdmin
import org.apache.hadoop.hbase.client.HTable
import org.apache.hadoop.hbase.client.Put
import org.apache.hadoop.hbase.client.Result
import org.apache.hadoop.hbase.io.ImmutableBytesWritable
import org.apache.hadoop.hbase.mapreduce.TableInputFormat
import org.apache.hadoop.hbase.util.Bytes

import akka.event.slf4j.Logger
import scala.collection.immutable.Seq
import org.apache.hadoop.hbase.client.Get
import org.apache.hadoop.hbase.client.Scan

/**
 * 使用spark-graphx实现基于好友、圈子来实现推荐二度好友、圈子、物品
 * 		要求：1.离线的、全量的；
 * 				 2.各种关系基于不同权重进行推荐
 *
 */
object RecBasedGraphx extends Serializable {
  val logger = Logger(this.getClass.getName)
  //def startRec (){
  def main(args: Array[String]): Unit = {
    val t1 = System.currentTimeMillis()
    //创建图
    val cg = new CreateGraph()  
    val graph = cg.createGraph(args(0), args(1),args(2)).cache()
    val t2 = System.currentTimeMillis()  
    logger.warn("创建图用时：" + (t2 - t1))
    //******************************推荐*****************************************    
    val recCode = RecCode
    //基于好友的推荐，包括推荐好友、物品和圈子，返回值类型是;RDD[(String, String, Map[String, List[(String, Double)]])]
    val recBasedFriends = recCode.recBasedSimUser(graph)
    val t3 = System.currentTimeMillis()  
    logger.warn("基于好友推荐用时：" + (t3 - t2))
    //离线的对每个用户基于圈子进行物品、好友和圈子的推荐,返回值是RDD[(String, String, Map[String, List[(String, Double)]])]
    val recBasedCircle = recCode.recICUForUserBasedCircle1(graph)
    val t4 = System.currentTimeMillis()
    logger.warn("基于圈子推荐用时：" + (t4 - t3))
    //对两种推荐结果进行合并   INFO.LOGINID:112233xyf;
    val recResult = recCode.combineResult(graph, "g1", recBasedFriends, recBasedCircle)
    //recResult.foreach(x => println("命名空间 "+x._1+" 下的用户："+x._2+" 的推荐结果是："+x._3.mkString(",")))
    val t5 = System.currentTimeMillis()
    logger.warn("合并推荐结果用时：" + (t5 - t4))
    
    //********************************关闭sc******************************************************
    OperateSparkContext.stop()
    //********************************将结果转化为json格式******************************************
    val dataToJson = new DataToJson()
    //将推荐结果中的对每个用户的推荐结果组成的Map中的list转化为json格式
    //数据格式：Array[(String, String, JSONObject)]-->Array[(表名, 用户业务id, JSONObject)]
    val jsonResult = recResult.collect().map(x => (x._1, x._2, dataToJson.mapToJson(x._3)))
    val t6 = System.currentTimeMillis()
    logger.warn("将推荐结果转化为json用时：" + (t6 - t5))
    //********************************将结果放到kafka中*********************************************
    val kafkaProducer = Producer
    //kafkaProducer.send("TEST_LOGINLOG", "qq", "xx"+"ss"+"22"+";")
    jsonResult.foreach { x =>
      kafkaProducer.send("TEST_LOGINLOG", x._2, x._3.toString())
    }
    val t7 = System.currentTimeMillis()
    logger.warn("将消息放到kafka中用时：" + (t7 - t6))
  }  
}

